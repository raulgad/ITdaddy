{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jiKZ8TYFKBw"
      },
      "outputs": [],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hnVtamfkWJ_R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import transformers\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAb1hQgNWKCE"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-doHWhpfFLn",
        "outputId": "e3f10ba3-14f8-4a3b-e279-a8d173bdd21c"
      },
      "outputs": [],
      "source": [
        "with open('IT_Advices_Proverbs_En.txt', encoding='utf8') as f:\n",
        "    text = f.read()\n",
        "    text = text.split('\\n')\n",
        "\n",
        "random.shuffle(text)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "646utEIIibXz",
        "outputId": "1139c0ed-ee16-469f-c8d6-af2ccb6769c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(train), len(test):  151 37\n",
            "max_length, index_max_length:  69 18\n"
          ]
        }
      ],
      "source": [
        "train = []\n",
        "test = []\n",
        "max_length = 0\n",
        "index_max_length = 0\n",
        "\n",
        "for idx, value in enumerate(text):\n",
        "  tokens = tokenizer.encode(value, add_special_tokens=True)\n",
        "  tokens = np.array(tokens)\n",
        "\n",
        "  curr_len = len(tokens)\n",
        "  if curr_len >= max_length:\n",
        "    max_length = curr_len\n",
        "    index_max_length = idx\n",
        "\n",
        "  if idx <= (len(text) * .8):\n",
        "    train.append(tokens)\n",
        "\n",
        "  else:\n",
        "    test.append(tokens)\n",
        "\n",
        "train = np.array(train)\n",
        "test = np.array(test)\n",
        "\n",
        "print('len(train), len(test): ', len(train), len(test))\n",
        "print('max_length, index_max_length: ', max_length, index_max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3rGTVpmnAgL",
        "outputId": "68485300-0b33-4109-b0ab-312b2be71b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(151, 69)\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "  3987   470  9585  3511   784   318   257  7989   286  3788  2478  8998\n",
            "   379  8868 29693   286  3788  7572    11 13586   340   351 12531   507\n",
            "   393  1262  1366  3487  1634   284  3368 49052    13]\n"
          ]
        }
      ],
      "source": [
        "def Padding(review_int, seq_len):\n",
        "    '''\n",
        "    Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
        "    '''\n",
        "    features = np.zeros((len(review_int), seq_len), dtype = int)\n",
        "    for i, review in enumerate(review_int):\n",
        "        if len(review) <= seq_len:\n",
        "            zeros = list(np.zeros(seq_len - len(review)))\n",
        "            # print(len(zeros))\n",
        "            # print(review.shape)\n",
        "            # new = np.array(zeros) + review\n",
        "            new = np.append(zeros, review)\n",
        "\n",
        "        features[i, :] = np.array(new)\n",
        "            \n",
        "    return features\n",
        "\n",
        "train = Padding(train, max_length)\n",
        "test = Padding(test, max_length)\n",
        "\n",
        "print(train.shape)\n",
        "print(train[105, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fLsB31NUfFOX"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, AdamW\n",
        "\n",
        "model_init = GPT2LMHeadModel.from_pretrained(\n",
        "    'gpt2',\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    'gpt2',\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.to(device);\n",
        "model_init.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9xwPiy5fFQw",
        "outputId": "85539765-4b2c-47a1-c8ad-9ea28839020f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19 5\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 1e-5, eps = 1e-8)\n",
        "\n",
        "batch_size = 8\n",
        "epochs = 100\n",
        "\n",
        "n_train = len(train)//batch_size + 1\n",
        "n_test = len(test)//batch_size + 1\n",
        "print(n_train, n_test)\n",
        "\n",
        "total_steps = n_train * epochs\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "def accuracy(y_true, logits):\n",
        "    return torch.mean((y_true[1:] == torch.argmax(logits, dim=2)[:-1]).float()).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "019bkBx11jL1",
        "outputId": "e42f79a7-2b5e-4cd2-a270-c7eb2e1a2c8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preped shape:  torch.Size([8, 69])\n"
          ]
        }
      ],
      "source": [
        "def prep_tensors(x, i, batch_size=batch_size):\n",
        "    start_idx = i*batch_size\n",
        "    end_idx = start_idx + batch_size\n",
        "    batch_ids = x[start_idx: end_idx]\n",
        "    batch_ids = torch.tensor(batch_ids).to(device)\n",
        "    return torch.tensor(batch_ids).to(device)\n",
        "\n",
        "preped = prep_tensors(train, 17)\n",
        "print('preped shape: ', preped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fr3PSnOhr2E"
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(f'epoch {epoch}/{epochs} : training')\n",
        "\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(range(n_train))\n",
        "    for i in pbar:\n",
        "        batch_ids = prep_tensors(train, i)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss, logits, _ = model(batch_ids,\n",
        "                             token_type_ids=None, \n",
        "                             labels=batch_ids\n",
        "                             ).values()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        train_loss.append(loss.item())\n",
        "        train_acc.append(accuracy(batch_ids, logits))\n",
        "        pbar.set_description(f'acc {np.mean(train_acc):.4f} loss {np.mean(train_loss):.4f}', refresh=True)\n",
        "\n",
        "    \n",
        "    print(f'epoch {epoch}/{epochs} : validation')\n",
        "    model.eval()\n",
        "    val_acc = []\n",
        "    val_loss = []\n",
        "    pbar = tqdm(range(n_test))\n",
        "    for i in pbar:\n",
        "        batch_ids = prep_tensors(test, i)\n",
        "        with torch.no_grad():        \n",
        "            loss, logits, _ = model(batch_ids, \n",
        "                                token_type_ids=None, \n",
        "                                labels=batch_ids\n",
        "                                 ).values()\n",
        "        \n",
        "        val_loss.append(loss.item())\n",
        "        val_acc.append(accuracy(batch_ids, logits))\n",
        "        pbar.set_description(f'acc {np.mean(val_acc):.4f} loss {np.mean(val_loss):.4f}', refresh=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhfZJYsBhr4g",
        "outputId": "8ba82b3d-f8d4-4a32-cf5f-631a38f12f08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Design must be used in conjunction with the following: (a) The name of the person to whom the application is made; and\n",
            "(b) the name, address, and telephone number of.\n",
            "------------------\n",
            "Design must be used in conjunction with the following: (a) The name and address of the person to whom the application\n",
            "is made; and (b) the name, address, and telephone.\n",
            "------------------\n",
            "Design must be used in conjunction with the following: (a) The name and address of the person to whom the application\n",
            "is made; and (b) the name, address, and telephone.\n",
            "------------------\n",
            "Design must be used in conjunction with the following: (a) The name and address of the person to whom the application\n",
            "is made; and (b) the name, address and telephone number.\n",
            "------------------\n",
            "Design must be used in conjunction with the following: (1) The name of the person to whom the application is made; and\n",
            "(2) the name, address, and telephone number of.\n",
            "------------------\n",
            "Design must be approved by the Secretary of Health and Human Services.\n",
            "------------------\n",
            "Design must be used in conjunction with the following: (a) The name and address of the person to whom the application\n",
            "is made; and (b) the name, address, and telephone.\n",
            "------------------\n"
          ]
        }
      ],
      "source": [
        "# Model without training\n",
        "\n",
        "prompt = 'Design must be'\n",
        "prompt = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "out = model_init.generate(\n",
        "    input_ids=prompt,\n",
        "    max_length=40,\n",
        "    num_beams=5,\n",
        "    do_sample=True,\n",
        "    temperature=.7,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_return_sequences=7,\n",
        "    ).cpu().numpy()\n",
        "for out_ in out:\n",
        "  wraped = textwrap.fill(tokenizer.decode(out_), 120)\n",
        "  wraped = wraped.replace(\"  \", \" \")\n",
        "  if '.' in wraped:\n",
        "    arr = wraped.split('.')\n",
        "    arr.pop()\n",
        "    final_out_text = '.'.join(arr) + '.'\n",
        "  else:\n",
        "    final_out_text = wraped + '.'\n",
        "  print(final_out_text, end='\\n------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqMenMhbAsLf",
        "outputId": "603c9773-89a9-49fd-8c0d-8b5f9d984ff1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Design must be kept in mind when it comes to the design of a computer system.\n",
            "------------------\n",
            "Design must be implemented in a way that is easy to read, maintain, and maintain.\n",
            "------------------\n",
            "Design must be implemented in a way that is easy to read, maintain, and maintain.\n",
            "------------------\n",
            "Design must be kept in mind when designing software, because it is often the first step in the development of a new\n",
            "product or service.\n",
            "------------------\n",
            "Design must be implemented in a way that is easy to read, maintain, and maintain.\n",
            "------------------\n",
            "Design must be kept in mind when designating a part of a system as a whole, because part-whole systems can vary widely\n",
            "in their functionality. Also, parts may not always be interchangeable.\n",
            "------------------\n",
            "Design must be kept in mind when it comes to the design of a computer system.\n",
            "------------------\n"
          ]
        }
      ],
      "source": [
        "# Trained model\n",
        "\n",
        "prompt = 'Design must be'\n",
        "prompt = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "out = model.generate(\n",
        "    input_ids=prompt,\n",
        "    max_length=40,\n",
        "    num_beams=5,\n",
        "    do_sample=True,\n",
        "    temperature=.7,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_return_sequences=7,\n",
        "    ).cpu().numpy()\n",
        "\n",
        "for out_ in out:\n",
        "  wraped = textwrap.fill(tokenizer.decode(out_), 120)\n",
        "  # print(wraped)\n",
        "  wraped = wraped.replace(\"  \", \" \")\n",
        "  if '.' in wraped:\n",
        "    arr = wraped.split('.')\n",
        "    arr.pop()\n",
        "    final_out_text = '.'.join(arr) + '.'\n",
        "  else:\n",
        "    final_out_text = wraped + '.'\n",
        "  print(final_out_text, end='\\n------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ggZEAtXekBU"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, len_gen=20, temperature=.7):\n",
        "    generated = tokenizer.encode(prompt)\n",
        "    context = torch.tensor([generated]).to(device)\n",
        "    past = None\n",
        "\n",
        "    for i in tqdm(range(len_gen)):\n",
        "        output, past = model(context, past_key_values=past).values()\n",
        "        # token = torch.argmax(output[..., -1, :], dim=-1)\n",
        "        output = output / temperature\n",
        "        token = torch.distributions.Categorical(logits=output[..., -1, :]).sample()\n",
        "        \n",
        "        generated += token.tolist()\n",
        "        context = token.unsqueeze(0)\n",
        "\n",
        "    sequence = tokenizer.decode(generated)\n",
        "\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWIT34VnAXzs"
      },
      "outputs": [],
      "source": [
        "prompt = 'Design must be'\n",
        "print(textwrap.fill(generate(prompt, 200, temperature=.8), 120))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Generate_IT_advices_en.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
