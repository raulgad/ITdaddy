{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jiKZ8TYFKBw"
      },
      "outputs": [],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hnVtamfkWJ_R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import transformers\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tCxKhSuD-2mQ",
        "outputId": "c5282333-e75f-4e13-cac5-3506e0a76ae0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.10.0+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAb1hQgNWKCE"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-doHWhpfFLn"
      },
      "outputs": [],
      "source": [
        "with open('IT_Advices_Proverbs_En.txt', encoding='utf8') as f:\n",
        "    text = f.read()\n",
        "    text = text.split('\\n')\n",
        "\n",
        "random.shuffle(text)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "646utEIIibXz",
        "outputId": "e086aa00-6345-4ade-f2be-931be0d50397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train), len(test):  151 37\n",
            "max_length, index_max_length:  69 132\n"
          ]
        }
      ],
      "source": [
        "train = []\n",
        "test = []\n",
        "max_length = 0\n",
        "index_max_length = 0\n",
        "\n",
        "for idx, value in enumerate(text):\n",
        "  tokens = tokenizer.encode(value, add_special_tokens=True)\n",
        "  tokens = np.array(tokens)\n",
        "\n",
        "  curr_len = len(tokens)\n",
        "  if curr_len >= max_length:\n",
        "    max_length = curr_len\n",
        "    index_max_length = idx\n",
        "\n",
        "  if idx <= (len(text) * .8):\n",
        "    train.append(tokens)\n",
        "\n",
        "  else:\n",
        "    test.append(tokens)\n",
        "\n",
        "train = np.array(train)\n",
        "test = np.array(test)\n",
        "\n",
        "print('len(train), len(test): ', len(train), len(test))\n",
        "print('max_length, index_max_length: ', max_length, index_max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3rGTVpmnAgL",
        "outputId": "a9ce16a1-c272-4ff0-a66e-dbb6a0111b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(151, 69)\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0 48032   416 15221\n",
            "   357  4090 14304  2599  8549  5361   351   262  8568  2962   319 26610\n",
            "  4542  9987    11   884   355  1271   286  4200    11   618   777   389\n",
            "  1729    12 31195   393  1575  1165   881   284 12831]\n"
          ]
        }
      ],
      "source": [
        "def Padding(review_int, seq_len):\n",
        "    '''\n",
        "    Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
        "    '''\n",
        "    features = np.zeros((len(review_int), seq_len), dtype = int)\n",
        "    for i, review in enumerate(review_int):\n",
        "        if len(review) <= seq_len:\n",
        "            zeros = list(np.zeros(seq_len - len(review)))\n",
        "            # print(len(zeros))\n",
        "            # print(review.shape)\n",
        "            # new = np.array(zeros) + review\n",
        "            new = np.append(zeros, review)\n",
        "\n",
        "        features[i, :] = np.array(new)\n",
        "            \n",
        "    return features\n",
        "\n",
        "train = Padding(train, max_length)\n",
        "test = Padding(test, max_length)\n",
        "\n",
        "print(train.shape)\n",
        "print(train[105, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLsB31NUfFOX"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, AdamW\n",
        "\n",
        "model_init = GPT2LMHeadModel.from_pretrained(\n",
        "    'gpt2',\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    'gpt2',\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.to(device);\n",
        "model_init.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9xwPiy5fFQw",
        "outputId": "a4bc993f-6d9a-473f-ffef-5df821edd88d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19 5\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 1e-5, eps = 1e-8)\n",
        "\n",
        "batch_size = 8\n",
        "epochs = 100\n",
        "\n",
        "n_train = len(train)//batch_size + 1\n",
        "n_test = len(test)//batch_size + 1\n",
        "print(n_train, n_test)\n",
        "\n",
        "total_steps = n_train * epochs\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "def accuracy(y_true, logits):\n",
        "    return torch.mean((y_true[1:] == torch.argmax(logits, dim=2)[:-1]).float()).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "019bkBx11jL1",
        "outputId": "c0bd8af3-e4fb-4308-ff3b-62a7829e82d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preped shape:  torch.Size([8, 69])\n"
          ]
        }
      ],
      "source": [
        "def prep_tensors(x, i, batch_size=batch_size):\n",
        "    start_idx = i*batch_size\n",
        "    end_idx = start_idx + batch_size\n",
        "    batch_ids = x[start_idx: end_idx]\n",
        "    batch_ids = torch.tensor(batch_ids).to(device)\n",
        "    return torch.tensor(batch_ids).to(device)\n",
        "\n",
        "preped = prep_tensors(train, 17)\n",
        "print('preped shape: ', preped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fr3PSnOhr2E"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, epochs+1):\n",
        "    print(f'epoch {epoch}/{epochs} : training')\n",
        "\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(range(n_train))\n",
        "    for i in pbar:\n",
        "        batch_ids = prep_tensors(train, i)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss, logits, _ = model(batch_ids,\n",
        "                             token_type_ids=None, \n",
        "                             labels=batch_ids\n",
        "                             ).values()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        train_loss.append(loss.item())\n",
        "        train_acc.append(accuracy(batch_ids, logits))\n",
        "        pbar.set_description(f'acc {np.mean(train_acc):.4f} loss {np.mean(train_loss):.4f}', refresh=True)\n",
        "\n",
        "    \n",
        "    print(f'epoch {epoch}/{epochs} : validation')\n",
        "    model.eval()\n",
        "    val_acc = []\n",
        "    val_loss = []\n",
        "    pbar = tqdm(range(n_test))\n",
        "    for i in pbar:\n",
        "        batch_ids = prep_tensors(test, i)\n",
        "        with torch.no_grad():        \n",
        "            loss, logits, _ = model(batch_ids, \n",
        "                                token_type_ids=None, \n",
        "                                labels=batch_ids\n",
        "                                 ).values()\n",
        "        \n",
        "        val_loss.append(loss.item())\n",
        "        val_acc.append(accuracy(batch_ids, logits))\n",
        "        pbar.set_description(f'acc {np.mean(val_acc):.4f} loss {np.mean(val_loss):.4f}', refresh=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhfZJYsBhr4g",
        "outputId": "2ad482d7-d637-4451-faa1-7efb6e534bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Design must be used with caution. If you have any questions, please contact us.\n",
            "------------------\n",
            "Design must be used in conjunction with the following: (a) The name and address of the person to whom the application\n",
            "is made; and (b) the name, address, and telephone.\n",
            "------------------\n",
            "Design must be used in conjunction with the following: (a) The name and address of the person to whom the application\n",
            "is made; and (b) the name, address, and telephone.\n",
            "------------------\n",
            "Design must be used in conjunction with the following: (a) The name and address of the person or persons to whom the\n",
            "application is made; and (b) A statement that the applicant.\n",
            "------------------\n",
            "Design must be approved by the Secretary of State.\n",
            "------------------\n",
            "Design must be used in conjunction with the following: (a) The name of the person to whom the application is made; and\n",
            "(b) the name, address, and telephone number of.\n",
            "------------------\n",
            "Design must be able to read, write, and interact with the content of the page.\n",
            "------------------\n"
          ]
        }
      ],
      "source": [
        "# Model without training\n",
        "\n",
        "prompt = 'Design must be'\n",
        "prompt = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "out = model_init.generate(\n",
        "    input_ids=prompt,\n",
        "    max_length=40,\n",
        "    num_beams=5,\n",
        "    do_sample=True,\n",
        "    temperature=.7,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_return_sequences=7,\n",
        "    ).cpu().numpy()\n",
        "for out_ in out:\n",
        "  wraped = textwrap.fill(tokenizer.decode(out_), 120)\n",
        "  wraped = wraped.replace(\"  \", \" \")\n",
        "  if '.' in wraped:\n",
        "    arr = wraped.split('.')\n",
        "    arr.pop()\n",
        "    final_out_text = '.'.join(arr) + '.'\n",
        "  else:\n",
        "    final_out_text = wraped + '.'\n",
        "  print(final_out_text, end='\\n------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqMenMhbAsLf",
        "outputId": "0efb42e3-573a-4b13-b6a9-9ca8ed773bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Design must be implemented in a manner that is least likely to break the flow of control or compromise the integrity of\n",
            "the system or the operation of a computer system, or both.\n",
            "------------------\n",
            "Design must be implemented in a manner that is least likely to break the flow of control or compromise the integrity of\n",
            "the system or the operation of a computer system, or both.\n",
            "------------------\n",
            "Design must be kept in mind when making decisions about which services to support, and which resources to allocate.\n",
            "------------------\n",
            "Design must be implemented in a manner that is least likely to break the flow of control or compromise the integrity of\n",
            "the system or the operation of a computer system, or both.\n",
            "------------------\n",
            "Design must be implemented in a manner that is least likely to break the flow of control or compromise the integrity of\n",
            "the system or the operation of a computer system, or both.\n",
            "------------------\n",
            "Design must be kept to a minimum, and must not be used to make decisions about the use of resources.\n",
            "------------------\n",
            "Design must be kept in mind when making decisions about which services to support, and which resources to allocate.\n",
            "------------------\n"
          ]
        }
      ],
      "source": [
        "# Trained model\n",
        "\n",
        "prompt = 'Design must be'\n",
        "prompt = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "out = model.generate(\n",
        "    input_ids=prompt,\n",
        "    max_length=40,\n",
        "    num_beams=5,\n",
        "    do_sample=True,\n",
        "    temperature=.7,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_return_sequences=7,\n",
        "    ).cpu().numpy()\n",
        "\n",
        "for out_ in out:\n",
        "  wraped = textwrap.fill(tokenizer.decode(out_), 120)\n",
        "  # print(wraped)\n",
        "  wraped = wraped.replace(\"  \", \" \")\n",
        "  if '.' in wraped:\n",
        "    arr = wraped.split('.')\n",
        "    arr.pop()\n",
        "    final_out_text = '.'.join(arr) + '.'\n",
        "  else:\n",
        "    final_out_text = wraped + '.'\n",
        "  print(final_out_text, end='\\n------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ggZEAtXekBU"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, len_gen=20, temperature=.7):\n",
        "    generated = tokenizer.encode(prompt)\n",
        "    context = torch.tensor([generated]).to(device)\n",
        "    past = None\n",
        "\n",
        "    for i in tqdm(range(len_gen)):\n",
        "        output, past = model(context, past_key_values=past).values()\n",
        "        # token = torch.argmax(output[..., -1, :], dim=-1)\n",
        "        output = output / temperature\n",
        "        token = torch.distributions.Categorical(logits=output[..., -1, :]).sample()\n",
        "        \n",
        "        generated += token.tolist()\n",
        "        context = token.unsqueeze(0)\n",
        "\n",
        "    sequence = tokenizer.decode(generated)\n",
        "\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWIT34VnAXzs"
      },
      "outputs": [],
      "source": [
        "prompt = 'Design must be'\n",
        "print(textwrap.fill(generate(prompt, 200, temperature=.8), 120))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model\n",
        "torch.save(model, 'itdaddy_en_model.pth')"
      ],
      "metadata": {
        "id": "FWgvQ8wTZlIN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's params\n",
        "torch.save(model.state_dict(), 'itdaddy_en_params.pth')"
      ],
      "metadata": {
        "id": "e6wNBtsuZ1TG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GEUiEeSg2jdZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Generate_IT_advices_en.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}